{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load FMNIST DATA"
      ],
      "metadata": {
        "id": "J6VD8tQx6J_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trhxjOCIle0C",
        "outputId": "4cac623e-6e12-4522-f8ad-46c784efb717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images.shape=torch.Size([128, 1, 28, 28]) labels.shape=torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import FashionMNIST, EMNIST\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "\n",
        "\n",
        "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
        "fdataset = FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(fdataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "\n",
        "images, labels = next(iter(dataloader))\n",
        "print(f\"{images.shape=} {labels.shape=}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch; torch.manual_seed(0)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200"
      ],
      "metadata": {
        "id": "g1vIx2JCn0RK"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "YyxsrUoBqo0s"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP ENCODER ARCHITECURE"
      ],
      "metadata": {
        "id": "TItBkyH76OIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(784, 512)\n",
        "        self.linear2 = nn.Linear(512, latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        return self.linear2(x)"
      ],
      "metadata": {
        "id": "K3bZlq55qtqx"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP ECODER ARCHITECTURE"
      ],
      "metadata": {
        "id": "boLLiPVZ6T72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(latent_dims, 512)\n",
        "        self.linear2 = nn.Linear(512, 784)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = F.relu(self.linear1(z))\n",
        "        z = torch.sigmoid(self.linear2(z))\n",
        "        return z.reshape((-1, 1, 28, 28))"
      ],
      "metadata": {
        "id": "JmNFhDa1qv80"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims)\n",
        "        self.decoder = Decoder(latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "oWRQ_yTYqzZB"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LATENT DIMENSIONS ARE SET TO 2"
      ],
      "metadata": {
        "id": "3RVdyeBm6ZCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dims = 2\n"
      ],
      "metadata": {
        "id": "uml1NhJuq2Bf"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39gi5vNDrCPD"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP VARIATIONAL ENCODER "
      ],
      "metadata": {
        "id": "HKNoNscq6biW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalEncoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(VariationalEncoder, self).__init__()\n",
        "        self.linear1 = nn.Linear(784, 512)\n",
        "        self.linear2 = nn.Linear(512, latent_dims)\n",
        "        self.linear3 = nn.Linear(512, latent_dims)\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
        "        self.N.scale = self.N.scale.cuda()\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        mu =  self.linear2(x)\n",
        "        sigma = torch.exp(self.linear3(x))\n",
        "        z = mu + sigma*self.N.sample(mu.shape)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
        "        return z"
      ],
      "metadata": {
        "id": "iovSSyr3rpj5"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP VARIATIONAL AUTOENCODER"
      ],
      "metadata": {
        "id": "itEv2UQ36e1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = VariationalEncoder(latent_dims)\n",
        "        self.decoder = Decoder(latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "V4n_UrZo5TZI"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING SCRIPT"
      ],
      "metadata": {
        "id": "RSGHBk-I6hUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(autoencoder, data, epochs=20):\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    for epoch in range(epochs):\n",
        "        for x, y in data:\n",
        "            x = x.to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = autoencoder(x)\n",
        "            loss = ((x - x_hat)**2).sum() + autoencoder.encoder.kl\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "j7kLCV4hrvVz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VariationalAutoencoder(latent_dims).to(device) # GPU\n",
        "vae = train(vae, dataloader)"
      ],
      "metadata": {
        "id": "jnlU94W_rxnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOTTING LATENT REPRESENTATION OF IMAGES"
      ],
      "metadata": {
        "id": "tml1XqJn6i_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent(autoencoder, data, num_batches=100):\n",
        "    for i, (x, y) in enumerate(data):\n",
        "        z = autoencoder.encoder(x.to(device))\n",
        "        z = z.to('cpu').detach().numpy()\n",
        "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
        "        if i > num_batches:\n",
        "            plt.colorbar()\n",
        "            break"
      ],
      "metadata": {
        "id": "4eGnQeZftXSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent(vae, dataloader)"
      ],
      "metadata": {
        "id": "1e8n6b0Wr1IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLOTTING RECONSTRUCTED IMAGES SYNTHESIZED FROM VAE"
      ],
      "metadata": {
        "id": "5_rBror06m0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reconstructed(autoencoder, r0=(-5, 10), r1=(-10, 5), n=12):\n",
        "    w = 28\n",
        "    img = np.zeros((n*w, n*w))\n",
        "    for i, y in enumerate(np.linspace(*r1, n)):\n",
        "        for j, x in enumerate(np.linspace(*r0, n)):\n",
        "            z = torch.Tensor([[x, y]]).to(device)\n",
        "            x_hat = autoencoder.decoder(z)\n",
        "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
        "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
        "    plt.imshow(img, extent=[*r0, *r1])"
      ],
      "metadata": {
        "id": "NdBj3H9QtXq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_reconstructed(vae, r0=(-3, 3), r1=(-3, 3))"
      ],
      "metadata": {
        "id": "_2ey5M9VtkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate(autoencoder, x_1, x_2, n=12):\n",
        "    z_1 = autoencoder.encoder(x_1)\n",
        "    z_2 = autoencoder.encoder(x_2)\n",
        "    z = torch.stack([z_1 + (z_2 - z_1)*t for t in np.linspace(0, 1, n)])\n",
        "    interpolate_list = autoencoder.decoder(z)\n",
        "    interpolate_list = interpolate_list.to('cpu').detach().numpy()\n",
        "\n",
        "    w = 28\n",
        "    img = np.zeros((w, n*w))\n",
        "    for i, x_hat in enumerate(interpolate_list):\n",
        "        img[:, i*w:(i+1)*w] = x_hat.reshape(28, 28)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "metadata": {
        "id": "b8CDM6_ytmLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(dataloader)) # hack to grab a batch\n",
        "x_1 = x[y == 1][1].to(device) # find a 1\n",
        "x_2 = x[y == 0][1].to(device) # find a 0iter()"
      ],
      "metadata": {
        "id": "5e7pt6Uwu4mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate(vae, x_1, x_2, n=20)"
      ],
      "metadata": {
        "id": "mFwaaF8ju650"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egFdJ67kyfrm"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}